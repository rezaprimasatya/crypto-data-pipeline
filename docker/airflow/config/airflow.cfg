[core]
# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor
executor = LocalExecutor

load_examples = False
# fernet_key = AIRFLOW__CORE__FERNET_KEY
fernet_key = tsJjtESQbN_24ADlMX2HISyIVwfj7pW1nEfYDkcPYMY=
hide_sensitive_variable_fields = True

[database]
# sql_alchemy_conn = AIRFLOW__CORE__SQL_ALCHEMY_CONN from manifest
sql_alchemy_conn = postgresql+psycopg2://airflow_user:a1rfl0w@airflow-database:5432/airflow_db

[cli]
api_client = airflow.api.client.local_client
endpoint_url = http://my-airflow.example.cool

[auth_backends]
auth_backend = airflow.api.auth.backend.default

[webserver]
base_url = http://my-airflow.example.cool
web_server_host = 0.0.0.0
web_server_port = 8080
# Set to true to turn on authentication:
# https://airflow.apache.org/security.html#web-authentication
authenticate = False
auth_backend = airflow.contrib.auth.backends.password_auth
# Use FAB-based webserver with RBAC feature
rbac = False
expose_config = True

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
run_duration = -1
min_file_process_interval = 5
dag_dir_list_interval = 30
print_stats_interval = 30
scheduler_health_check_threshold = 30
child_process_log_directory = /opt/airflow/logs/scheduler
scheduler_zombie_task_threshold = 30
catchup_by_default = True
max_tis_per_query = 512
parsing_processes = 2
authenticate = False
use_job_schedule = True

[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False
